{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#Initial Imports\n",
    "import gzip#; print('gzip version: ', gzip.__version__)\n",
    "import os#; print('os version: ', os.__version__)\n",
    "import re#; print('re version: ', re.__version__)\n",
    "import shutil#; print('shutil version: ', shutil.__version__)\n",
    "import requests#; print('requests version: ', requests.__version__)\n",
    "import wget#; print('wget version: ', wget.__version__)\n",
    "import subprocess#; print('subprocess version: ', subprocess.__version__)\n",
    "import time#; print('time version: ', time.__version__)\n",
    "from datetime import datetime\n",
    "from subprocess import PIPE, Popen\n",
    "from fake_useragent import UserAgent\n",
    "import os.path\n",
    "from os import path\n",
    "from Bio.ExPASy import Enzyme\n",
    "import numpy as np \n",
    "from sklearn.metrics import pairwise_distances\n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Unitprot.fasta for Use in Diamond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsv_to_fasta():\n",
    "    reference_library = 'uniprot.tsv' \n",
    "    #this creates a file named 'uniprot.tsv' on the working directory. Should show within the EcoDr/EcoDr folder. \n",
    "    ua = UserAgent()\n",
    "    header = {'User-Agent': str(ua.chrome)}\n",
    "    uniprot_url = 'https://rest.uniprot.org/uniprotkb/stream?fields=accession%2Cec%2Csequence&format=tsv&query=%28%28ec%3A*%29%29+AND+%28reviewed%3Atrue%29'\n",
    "    time.sleep(4)\n",
    "    uniprot = requests.get(uniprot_url, headers=header)\n",
    "    if uniprot.status_code == 200:\n",
    "        with open(reference_library, 'w+') as reference_library:\n",
    "            reference_library.write(uniprot.text)\n",
    "    print('Uniprot Reference File Has Been Created')\n",
    "####this step takes it the initial tsv and converts it to FASTA\n",
    "    input = os.path.abspath('uniprot.tsv')\n",
    "    output = 'uniprot.fasta'\n",
    "    with open(input, 'r') as input_file, open(output, 'w') as output_file:\n",
    "        header=next(input_file)\n",
    "        for line in input_file:\n",
    "            carrot = f'>{line}'\n",
    "            new_row = re.sub(r'(.*?\\t.*?)\\t', r'\\1\\n', carrot, 1)\n",
    "            new_row_with_tab_replaced_by_question_mark = new_row.replace(\"\\t\", \"?\")\n",
    "            # Be careful with the symbols used for replacement, as they have to align with the genomic summary string parsing\n",
    "            output_row_with_space_replaced_with_ampersand = new_row_with_tab_replaced_by_question_mark.replace(\"; \", \";_\")\n",
    "            #new_row2 = re.sub(r'(.*?\\t.*?)\\t', r'$', new_row, 0)\n",
    "            #this is regex, for the record. \n",
    "            output_file.write(output_row_with_space_replaced_with_ampersand)\n",
    "    print('FASTA has been created from TSV and is named', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EC List Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EC_extract():\n",
    "    ec_library = 'EC_library.csv' \n",
    "    ua = UserAgent()\n",
    "    header = {'User-Agent': str(ua.chrome)}\n",
    "    ec_url = 'https://ftp.expasy.org/databases/enzyme/enzyme.dat'\n",
    "    time.sleep(4)\n",
    "    ec = requests.get(ec_url, headers=header)\n",
    "    if ec.status_code == 200:\n",
    "        with open(ec_library, 'w+', newline='\\n') as ec_file:\n",
    "            ec_file.write(ec.text)\n",
    "###This step creates the list \n",
    "    handle = open(ec_library)\n",
    "    records = Enzyme.parse(handle)\n",
    "    ecnumbers = [record[\"ID\"] for record in records]\n",
    "    #print(type(ecnumbers)) #This is a list at this point in the code\n",
    "    path = os.path.abspath(ec_library)\n",
    "    with open(path, 'w+', newline='\\n') as csv_file:\n",
    "        #csv_file = csv.writer(csv_file)\n",
    "        for item in ecnumbers:\n",
    "            csv_file.write(item +'\\n')\n",
    "    print('EC list Has Been Created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diamond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diamond_impl(dest, name):\n",
    "    print(os.getcwd())\n",
    "    matches = ''\n",
    "    output_folder = dest \n",
    "    final_folder = '/home/anna/Documents/JGI_soil_genomes/diamond_analysis_output/'\n",
    "    print(\"DIAMOND search library is: \", output_folder)\n",
    "    if os.path.isfile('reference.dmnd'):\n",
    "        print(\"Library Detected\")\n",
    "    # If not present, then creates a DIAMOND library by referencing the exact location where the Uniprot library is saved\n",
    "    # If there currently is no reference library (.dmnd), then command makedb creates a DIAMOND library\n",
    "    else:\n",
    "        print(\"Creation of DIAMOND-formatted library...\")\n",
    "        makedb = ['diamond', 'makedb', '--in', '/home/anna/Documents/EnCen/uniprot.fasta', '-d',\n",
    "                  'Uniprot_Reference_Library.dmnd']  # Reference library full pathway\n",
    "        #This is a list for the DIAMOND specific makedb function. \n",
    "        subprocess.run(makedb)\n",
    "        #This simply runs the function makedb\n",
    "        print(\"Library complete\")\n",
    "#^this chunk is good to go. Just makes the reference database from the uniprot fasta. \n",
    "##_______________________________________________________________# This portion does the matching. The above portion creates the reference library from the unitprot fasta\n",
    "    for item in os.listdir(dest):\n",
    "        # Checks for file extension\n",
    "        if item.endswith('.faa') and not os.path.isfile(\n",
    "                os.path.basename(os.path.abspath(item)).rsplit('.', 1)[0] + \"_matches.tsv\"):\n",
    "            # Finds path of file\n",
    "            file_path = os.path.abspath(item)\n",
    "            # Finds the GCF/ASM name of the file by looking at the first part of the name before the .faa notation\n",
    "            if name == \"\":\n",
    "                print(os.path.basename(file_path).rsplit('.',1))\n",
    "                file_name = (os.path.basename(file_path)).rsplit('.', 1)[0]\n",
    "            else:\n",
    "                file_name = name\n",
    "            # New filename that ends with matches\n",
    "            matches = file_name + \"_matches.tsv\"\n",
    "            # print(matches)\n",
    "            # If genome has not already undergone DIAMOND search and is currently located in the correct folder, then\n",
    "            # the subprocess function will run the diamond search\n",
    "            if not os.path.isfile(dest + \"/\" + matches) and os.path.abspath(matches) != output_folder:\n",
    "                print(\"Processing \", file_name)\n",
    "                # DIAMOND search using the full pathway of the protein files, max target sequence outputs only one best\n",
    "                # match with highest e-value which represent the chance of obtaining a better random match in the same database (Buchfink et al, 2021)\n",
    "                blastp = ['diamond', 'blastp', '-d', 'Uniprot_Reference_Library.dmnd', '-q', file_path, '-o', matches, \n",
    "                          '--max-target-seqs', '1', '--outfmt', '6']\n",
    "                time.sleep(4)\n",
    "                subprocess.run(blastp)\n",
    "        # (2) Creates a folder for DIAMOND outputs\n",
    "        #if not os.path.exists(output_folder):\n",
    "            #os.makedirs('DIAMOND_matches')\n",
    "\n",
    "    # Moves all DIAMOND search outputs into the folder\n",
    "        if item.endswith('.tsv'):\n",
    "            if os.path.exists(os.path.join(final_folder, item)):\n",
    "                print(f\"Overwriting: {item}\")\n",
    "                os.remove(os.path.join(final_folder, item))\n",
    "            shutil.move(os.path.abspath(item), final_folder)\n",
    "    print(\"diamond_impl--success\")\n",
    "    # Returns the location of the DIAMOND matches folder\n",
    "    return output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genome_extractor(diamond_folder, name):\n",
    "    os.chdir(diamond_folder)\n",
    "    print(os.getcwd())\n",
    "    # Opens the list of of EC numbers\n",
    "    ec_open = np.loadtxt('/home/anna/Documents/JGI_soil_genomes/EC_library.csv',\n",
    "                         dtype='str')\n",
    "    big_matrix = [\"Name_of_Genome\"]\n",
    "    # Asks user to input name for EC matrix\n",
    "    # input_name = input(\"Save EC binary summary matrix as (no spaces or capital letters): \")\n",
    "    # Specifies document to be a csv type\n",
    "    # file_name= input_name+\".csv\"\n",
    "    # file_name = \"synbio1_big_matrix.csv\"\n",
    "    if name == \"\":\n",
    "        file_name = os.path.abspath(diamond_folder).rsplit('/', 9)[4] + '_binary_matrix.txt'\n",
    "        print(os.path.abspath(diamond_folder).rsplit('/', 9))\n",
    "        print(file_name)\n",
    "    else:\n",
    "        file_name = name\n",
    "    new_dir = diamond_folder + '/' + file_name\n",
    "    # Checks to see if the document already exists using full pathway name\n",
    "    if os.path.exists(new_dir):\n",
    "        pass\n",
    "        #print(\"Summary Matrix exists\")\n",
    "        #return [new_dir, file_name]\n",
    "    else:\n",
    "        for ec_force in ec_open:\n",
    "            # Creates a horizontal header of all of the EC names\n",
    "            big_matrix.append(ec_force)\n",
    "        # Goes through all of the DIAMOND outputs in the folder\n",
    "        # Goes through all of the output files, each one is opened and read one line at a time. The lines are split to\n",
    "        # extract the EC numbers found in each line. If the EC number found in the DIAMOND output matches an\n",
    "        # EC entry in the list, the status is changed from a zero to a one. The binary status is catalogued horizontally\n",
    "        # for each genome, and following genomes are vertically stacked\n",
    "        for item in os.listdir(diamond_folder):\n",
    "            if item.endswith(\"_matches.tsv\"):\n",
    "                print(item)\n",
    "                # Finds the name of the DIAMOND output file\n",
    "                genome = [item] #Turns the GCF's into a list, where the GCF names in the matrix come from. \n",
    "                genome_runner_ec = [item] #Turns the GCF's into a list, where the EC is appended\n",
    "                # Iterates through all of the EC numbers (1:8197)\n",
    "                \n",
    "                GCF = open(item, 'r') # CBM Added\n",
    "                \n",
    "                for line in GCF: # CBM Added\n",
    "                    no_tab = line.split('\\t')\n",
    "                    first_ec = no_tab[1].split(\"?\")\n",
    "                    separate_ec = first_ec[1].split(\";_\")\n",
    "                    genome_runner_ec.append(separate_ec)\n",
    "                    print(\"Appending...\")\n",
    "\n",
    "                for ec in ec_open:\n",
    "                    # print(\"EC we actually are looking for \"+ ec)\n",
    "                    # Opens individual DIAMOND output files\n",
    "                    #CBM GCF = open(item, 'r')\n",
    "                    # Sets default for EC status is zero, meaning absent\n",
    "                    #CBM ec_now = 0\n",
    "                    # Takes the first line in the DIAMOND output file and splits it based on tab separation\n",
    "                    # Takes the second column of the split line, which has EC numbers separated by a ?, ;_\n",
    "                    # Strings splits have a new name assigned to them\n",
    "                    #CBM for line in GCF:\n",
    "                    #CBM    print(line)\n",
    "                    #CBM    no_tab = line.split('\\t')\n",
    "                    #CBM    first_ec = no_tab[1].split(\"?\")\n",
    "                    #CBM    separate_ec = first_ec[1].split(\";_\")\n",
    "                    #CBM    print(\"Seperate EC Likely Nightmare \"+ separate_ec[0])\n",
    "                        # Checks for a full match between the EC number listed in the DIAMOND output and the EC number\n",
    "                        # found in the separate document\n",
    "                    #CBM    if re.fullmatch(ec, first_ec[1]) is not None:  # looks for full match of first EC number\n",
    "                    #CBM        ec_now = 1\n",
    "                        # In the case that there are more than one EC separated by ;, the function iterates through the list\n",
    "                        # and sees if there is a full match between the listed EC and the list\n",
    "                    #CBM    for i in separate_ec:\n",
    "                    #CBM        if re.fullmatch(ec, i) is not None:  # looks for full match of any other ECs listed\n",
    "                    #CBM            ec_now = 1\n",
    "                    ec_now = 0\n",
    "                    if [ec] in genome_runner_ec:\n",
    "                        ec_now = 1\n",
    "\n",
    "                    # 1 or 0 will be appended to the summary matrix for each EC value in the list\n",
    "                    genome.append(ec_now)\n",
    "                    #print(genome)\n",
    "                # Vertical stacking occurs for each genome in the DIAMOND output folder\n",
    "                big_matrix = np.vstack([big_matrix, genome])\n",
    "        #print(big_matrix)\n",
    "        # Saves matrix as a text file for further analysis\n",
    "        np.savetxt(file_name, big_matrix, fmt='%s')\n",
    "        # Returns the location of the summary matrix and the name of the file\n",
    "        if not os.path.exists('/home/anna/Documents/JGI_soil_genomes/JGI_soil_genomes_binary_matrix.txt'):\n",
    "            shutil.move(os.path.abspath('JGI_soil_genomes_binary_matrix.txt'),'/home/anna/Documents/JGI_soil_genomes')\n",
    "        else:\n",
    "            print('File already Exists')\n",
    "        print(new_dir)\n",
    "    return [new_dir, file_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I know things beyond the Maginot line are harried. \n",
      " But I worry. I've asked you time and time again. \n",
      "  Abandon your cataloging. \n",
      " Come Back Inside, where my fleets can keep you safe\n",
      "\n",
      " \n",
      " Come Home\n"
     ]
    }
   ],
   "source": [
    "print('I know things beyond the Maginot line are harried. \\n But I worry. I\\'ve asked you time and time again. \\n  Abandon your cataloging. \\n Come Back Inside, where my fleets can keep you safe')\n",
    "print('\\n \\n Come Home')\n",
    "metagenome_name = 'diamond_analysis_output' #-> folder\n",
    "desired_location = '/home/anna/Documents/JGI_soil_genomes' \n",
    "# Freshwater = nump/nump/nump/files\n",
    "soil = '/home/anna/Documents/JGI_soil_genomes'\n",
    "abspath = os.path.abspath(soil)\n",
    "\n",
    "# EC_extract()\n",
    "# tsv_to_fasta()\n",
    "\n",
    "#______________________________________________________________#\n",
    "os.chdir(desired_location) #-> we are in the folder we want\n",
    "if os.path.exists(metagenome_name):\n",
    "    shutil.rmtree(metagenome_name)\n",
    "    os.mkdir(metagenome_name)\n",
    "else:#makes a new directory called metagenome_name\n",
    "    os.mkdir(metagenome_name)\n",
    "desired_location = desired_location + \"/\" + metagenome_name #-> this will make a folder called 'metagenome_analysis_output' within the 'desired_location' \n",
    "# shutil.copy(abspath, desired_location) #moves  file to Test Cases folder\n",
    "#matches = metagenome + \"_matches\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamond = diamond_impl(soil, '') #-> Takes in the path and directory\n",
    "# new_dir, saved_file_name = genome_extractor(desired_location, '') #-> At this point, we have the one hotted binary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_folder = '/home/anna/Documents/JGI_soil_genomes/diamond_analysis_output'\n",
    "desired_location = '/home/anna/Documents/JGI_soil_genomes'\n",
    "# print(desired_location)\n",
    "for item in os.listdir(desired_location):\n",
    "    if item.endswith('_matches.tsv'):\n",
    "        source = os.path.join(desired_location, item)\n",
    "        destination = os.path.join(final_folder, item)\n",
    "        shutil.move(source, destination)\n",
    "    # if item.endswith('.tsv'):\n",
    "    #             if os.path.exists(os.path.join(final_folder, item)):\n",
    "    #                 print(f\"Overwriting: {item}\")\n",
    "    #                 os.remove(os.path.join(final_folder, item))\n",
    "    #             shutil.move(os.path.abspath(item), final_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anna/Documents/JGI_soil_genomes\n",
      "['', 'home', 'anna', 'Documents', 'JGI_soil_genomes']\n",
      "JGI_soil_genomes_binary_matrix.txt\n",
      "['/home/anna/Documents/JGI_soil_genomes/JGI_soil_genomes_binary_matrix.txt', 'JGI_soil_genomes_binary_matrix.txt']\n"
     ]
    }
   ],
   "source": [
    "output = genome_extractor(diamond, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We Now Have the functional profiles of each soil .faa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Next Steps is to make the functional profile out of the synbio.faa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "synbio = '/home/anna/Documents/JGI_soil_genomes/Synbio/'\n",
    "name = 'Synbio_Functional_Profile'\n",
    "desired_location2 = '/home/anna/Documents/JGI_soil_genomes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anna/Documents/JGI_soil_genomes/Synbio\n",
      "DIAMOND search library is:  /home/anna/Documents/JGI_soil_genomes/Synbio/\n",
      "Creation of DIAMOND-formatted library...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diamond v0.9.30.131 (C) Max Planck Society for the Advancement of Science\n",
      "Documentation, support and updates available at http://www.diamondsearch.org\n",
      "\n",
      "#CPU threads: 20\n",
      "Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)\n",
      "Database file: /home/anna/Documents/EnCen/uniprot.fasta\n",
      "Opening the database file...  [0.016s]\n",
      "Loading sequences...  [0.449s]\n",
      "Masking sequences...  [0.679s]\n",
      "Writing sequences...  [0.164s]\n",
      "Hashing sequences...  [0.082s]\n",
      "Loading sequences...  [0s]\n",
      "Writing trailer...  [0.006s]\n",
      "Closing the input file...  [0s]\n",
      "Closing the database file...  [0.052s]\n",
      "Database hash = 9ae9565aeffc045297d8444896762120\n",
      "Processed 275960 sequences, 114276793 letters.\n",
      "Total time = 1.45s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library complete\n",
      "Processing  Synbio_Functional_Profile\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diamond v0.9.30.131 (C) Max Planck Society for the Advancement of Science\n",
      "Documentation, support and updates available at http://www.diamondsearch.org\n",
      "\n",
      "#CPU threads: 20\n",
      "Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)\n",
      "Temporary directory: \n",
      "Opening the database...  [0s]\n",
      "#Target sequences to report alignments for: 1\n",
      "Reference = Uniprot_Reference_Library.dmnd\n",
      "Sequences = 275960\n",
      "Letters = 114276793\n",
      "Block size = 2000000000\n",
      "Opening the input file...  [0s]\n",
      "Opening the output file...  [0s]\n",
      "Loading query sequences...  [0.003s]\n",
      "Masking queries...  [0.015s]\n",
      "Building query seed set...  [0.004s]\n",
      "Algorithm: Double-indexed\n",
      "Building query histograms...  [0.001s]\n",
      "Allocating buffers...  [0s]\n",
      "Loading reference sequences...  [0.213s]\n",
      "Masking reference...  [0.666s]\n",
      "Initializing temporary storage...  [0s]\n",
      "Building reference histograms...  [0.227s]\n",
      "Allocating buffers...  [0s]\n",
      "Processing query block 0, reference block 0, shape 0, index chunk 0.\n",
      "Building reference seed array...  [0.161s]\n",
      "Building query seed array...  [0.002s]\n",
      "Computing hash join...  [0.034s]\n",
      "Building seed filter...  [0.001s]\n",
      "Searching alignments...  [0.011s]\n",
      "Processing query block 0, reference block 0, shape 0, index chunk 1.\n",
      "Building reference seed array...  [0.178s]\n",
      "Building query seed array...  [0.002s]\n",
      "Computing hash join...  [0.032s]\n",
      "Building seed filter...  [0.002s]\n",
      "Searching alignments...  [0.009s]\n",
      "Processing query block 0, reference block 0, shape 0, index chunk 2.\n",
      "Building reference seed array...  [0.186s]\n",
      "Building query seed array...  [0.002s]\n",
      "Computing hash join...  [0.034s]\n",
      "Building seed filter...  [0.001s]\n",
      "Searching alignments...  [0.009s]\n",
      "Processing query block 0, reference block 0, shape 0, index chunk 3.\n",
      "Building reference seed array...  [0.165s]\n",
      "Building query seed array...  [0.003s]\n",
      "Computing hash join...  [0.033s]\n",
      "Building seed filter...  [0.001s]\n",
      "Searching alignments...  [0.009s]\n",
      "Processing query block 0, reference block 0, shape 1, index chunk 0.\n",
      "Building reference seed array...  [0.135s]\n",
      "Building query seed array...  [0.002s]\n",
      "Computing hash join...  [0.032s]\n",
      "Building seed filter...  [0.001s]\n",
      "Searching alignments...  [0.008s]\n",
      "Processing query block 0, reference block 0, shape 1, index chunk 1.\n",
      "Building reference seed array...  [0.183s]\n",
      "Building query seed array...  [0.002s]\n",
      "Computing hash join...  [0.032s]\n",
      "Building seed filter...  [0.001s]\n",
      "Searching alignments...  [0.008s]\n",
      "Processing query block 0, reference block 0, shape 1, index chunk 2.\n",
      "Building reference seed array...  [0.207s]\n",
      "Building query seed array...  [0.003s]\n",
      "Computing hash join...  [0.031s]\n",
      "Building seed filter...  [0.001s]\n",
      "Searching alignments...  [0.007s]\n",
      "Processing query block 0, reference block 0, shape 1, index chunk 3.\n",
      "Building reference seed array...  [0.156s]\n",
      "Building query seed array...  [0.003s]\n",
      "Computing hash join...  [0.031s]\n",
      "Building seed filter...  [0s]\n",
      "Searching alignments...  [0.009s]\n",
      "Deallocating buffers...  [0.035s]\n",
      "Computing alignments... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diamond_impl--success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [0.231s]\n",
      "Deallocating reference...  [0.015s]\n",
      "Loading reference sequences...  [0s]\n",
      "Deallocating buffers...  [0s]\n",
      "Deallocating queries...  [0s]\n",
      "Loading query sequences...  [0s]\n",
      "Closing the input file...  [0s]\n",
      "Closing the output file...  [0s]\n",
      "Closing the database file...  [0s]\n",
      "Deallocating taxonomy...  [0s]\n",
      "Total time = 3.166s\n",
      "Reported 775 pairwise alignments, 775 HSPs.\n",
      "775 queries aligned.\n"
     ]
    }
   ],
   "source": [
    "os.chdir(synbio) \n",
    "diamond_syn = diamond_impl(synbio, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anna/Documents/JGI_soil_genomes/Synbio\n",
      "Synbio_Functional_Profile.txt_matches.tsv\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "Appending...\n",
      "File already Exists\n",
      "/home/anna/Documents/JGI_soil_genomes/Synbio//Synbio_Functional_Profile.txt\n",
      "['/home/anna/Documents/JGI_soil_genomes/Synbio//Synbio_Functional_Profile.txt', 'Synbio_Functional_Profile.txt']\n"
     ]
    }
   ],
   "source": [
    "output2 = genome_extractor(diamond_syn, name)\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synbio Functional Profile is now created. We now have both the environmental profile and the synbio profile. From Here down is distance scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anna/Documents/JGI_soil_genomes/Synbio//Synbio_Functional_Profile\n"
     ]
    }
   ],
   "source": [
    "synbio_binary = output2[0] #This is the synbio_functional_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genome_to_genome_diffcomp(synbio_binary, domain_binary):\n",
    "    names_of_orgs = pd.DataFrame(domain_binary.index) \n",
    "    diff = pd.DataFrame(abs(domain_binary.values - synbio_binary.values))\n",
    "    row_sum = diff.sum(axis=1)\n",
    "    df1 = pd.DataFrame(row_sum)\n",
    "    difference_based_comparison = pd.concat([names_of_orgs, df1], axis=1)\n",
    "    difference_based_comparison.columns = ['Organisms Compared to Synbio', 'Difference Score']\n",
    "    difference_based_comparison = difference_based_comparison.sort_values(by='Difference Score',\n",
    "                                                                          ignore_index=True).reset_index(drop=True)\n",
    "    difference_based_comparison.to_csv('Difference_Based_Comparison_Score.txt', header=True, index=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_binary_matrix(synbio_binary, sb_name):\n",
    "    # Converts synbio summary matrix into a dataframe\n",
    "    synbio_binary = pd.read_csv(synbio_binary, delimiter=\" \", header=0)\n",
    "    # print(synbio_binary)\n",
    "    synbio_binary = synbio_binary.set_index('Name_of_Genome')\n",
    "    #index is a label for all rows - allows the two seperate dataframes to come together since they share an index\n",
    "    # print(synbio_binary)\n",
    "    print(sb_name, \" size of \", np.shape(synbio_binary), \" successfully imported.\")\n",
    "    # Opens the matrix that includes the Bacteria and Archaea summary result\n",
    "    domain_binary = pd.read_csv('/home/anna/Documents/JGI_soil_genomes/JGI_soil_genomes_binary_matrix.txt',\n",
    "                                  delimiter=\" \", header=0)\n",
    "    #this is from chunk 1, and is the EC_Binary we generated earlier\n",
    "    domain_binary = domain_binary.set_index('Name_of_Genome')\n",
    "    print(domain_binary)\n",
    "    # Sends to a function for direct genome to genome comparison based on EC summary matrix\n",
    "    genome_to_genome_diffcomp(synbio_binary, domain_binary)\n",
    "    synbio_bacteria = pd.concat([domain_binary, synbio_binary])\n",
    "    # Verically adding synbio matrix to the overall matrix to complete the comparison, therefore last index\n",
    "    # should represent the synbio genome that is tested and returned as distances_mat[-1,:] in pass_to_distance.\n",
    "    # Note, headers are lost and need to be directly passed\n",
    "    # print(\"Shape of combined matrix using append is: \", np.shape(synbio_bacteria))\n",
    "    print(\"Merging of bacteria data and synbio data is complete\")\n",
    "    doc_name_1 = 'complete_binary_matrix.txt'\n",
    "    # Removes any duplicates\n",
    "    all_matrix = synbio_bacteria[~synbio_bacteria.index.duplicated(keep='first')]\n",
    "    print(all_matrix)\n",
    "    all_matrix.to_csv(doc_name_1, header=True, index=True, sep='\\t')\n",
    "    return all_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_distance(input_df, genome_names,genome_ID):\n",
    "    #calculating_distance(clustered_ec, list_genomes, clustering_pref, index_names, sb_name, rank)\n",
    "    # Calculate the distances of the datafile using the pdist funciton from scipy. The intial return will only\n",
    "    # provide the upper half of the comparisons (row-wise) to create a symetrical matrix then create squareform\n",
    "    name = genome_ID + \"_\" + \"_unclustered_\" + 'Distance_Matrix_Combined.txt'\n",
    "    distances_parallel = pairwise_distances(X=input_df, metric='euclidean', n_jobs=18)\n",
    "    #imported function that takes in the ec, calculates the distance between rows, n_jobs is for large datasets\n",
    "    print(\"Shape of the entire distance matrix is: \", np.shape(distances_parallel))\n",
    "    # NaNs at this stage may indicate header name mismatch - check if EC numbers are aligning\n",
    "    # If wanting to save the full distance matrix, then turn the following flag on. Note: Well above 10 GB\n",
    "    distances_parallel = pd.DataFrame(distances_parallel)\n",
    "    print(distances_parallel)\n",
    "    #print('Chocolate Sunday')\n",
    "    # Returns the full distance matrix for dendrogram construction in R script\n",
    "    #distances_parallel.to_csv('diff_weighted_unclustered_distance_matrix.txt', header=False, index=False, sep='\\t')\n",
    "    #distances_parallel.to_csv('both_chimeras_distance_matrix_clustered.txt', header=False, index=False, sep='\\t')\n",
    "    print(\"Distance matrix is downloaded\")\n",
    "    print(genome_names)\n",
    "    # Converts the distance matrix of synbio as a data frame. Concat binds the row names dataframe with the synbio distance\n",
    "    # matrix. Result should be a [2,#of total genomes]\n",
    "    distances_synbio = pd.concat([genome_names.reset_index(drop=True),\n",
    "                                    distances_parallel.reset_index(drop=True)], axis=1)\n",
    "    #axis=1 is the columns. Just adds genome names to the final score output\n",
    "    distances_parallel.index = genome_names['Name_of_Genome'].tolist()\n",
    "    distances_parallel.to_csv('Chimera1_unclustered_unweighted_DM.txt', header = True, index = True, sep='\\t')\n",
    "    #distances_parallel.set_index('Name_of_Genome', inplace=True, drop=True)\n",
    "    # Finds the row that contains the synbio genome based on genome ID\n",
    "    tsv_name = genome_ID.replace(\".faa\", \"\")\n",
    "    tsv_name2 = tsv_name + \"_matches.tsv\"\n",
    "    print(tsv_name2)\n",
    "    synbio_row = distances_parallel.loc[tsv_name2 ,:]\n",
    "    #grabs the name of the first GCF using the index\n",
    "    synbio_column = synbio_row.T\n",
    "    # Creates the vertical genome names for the resulting matrix\n",
    "    synbio_column.index = genome_names\n",
    "    # Sets column names\n",
    "    synbio_column.columns = ['Genome_Name', 'Calculated Distance']\n",
    "    return synbio_column, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_to_distance(synbio_binary, sb_name, desired_location):\n",
    "    all_matrix = read_in_binary_matrix(synbio_binary, sb_name)\n",
    "    list_genomes = pd.DataFrame(all_matrix.index)\n",
    "    # Completes distance calculation on an Euclidean basis. Returns the synbio column with genome names appended\n",
    "    [synbio_clustered_distances, name] = calculating_distance(all_matrix, list_genomes, sb_name)\n",
    "    \n",
    "    synbio_clustered_distances.to_csv(name, header=True, index=True, sep='\\t')\n",
    "    return synbio_clustered_distances, desired_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synbio_Functional_Profile  size of  (1, 8236)  successfully imported.\n",
      "                          1.1.1.1  1.1.1.2  1.1.1.3  1.1.1.4  1.1.1.5  \\\n",
      "Name_of_Genome                                                          \n",
      "2162886007_5_matches.tsv        1        0        1        0        0   \n",
      "3300000532_1_matches.tsv        1        0        1        0        0   \n",
      "\n",
      "                          1.1.1.6  1.1.1.7  1.1.1.8  1.1.1.9  1.1.1.10  ...  \\\n",
      "Name_of_Genome                                                          ...   \n",
      "2162886007_5_matches.tsv        0        0        0        0         1  ...   \n",
      "3300000532_1_matches.tsv        0        0        0        0         0  ...   \n",
      "\n",
      "                          7.6.2.7  7.6.2.8  7.6.2.9  7.6.2.10  7.6.2.11  \\\n",
      "Name_of_Genome                                                            \n",
      "2162886007_5_matches.tsv        0        0        0         1         1   \n",
      "3300000532_1_matches.tsv        0        1        0         1         1   \n",
      "\n",
      "                          7.6.2.12  7.6.2.13  7.6.2.14  7.6.2.15  7.6.2.16  \n",
      "Name_of_Genome                                                              \n",
      "2162886007_5_matches.tsv         1         0         0         0         0  \n",
      "3300000532_1_matches.tsv         0         0         0         0         0  \n",
      "\n",
      "[2 rows x 8236 columns]\n",
      "Merging of bacteria data and synbio data is complete\n",
      "                                       1.1.1.1  1.1.1.2  1.1.1.3  1.1.1.4  \\\n",
      "Name_of_Genome                                                              \n",
      "2162886007_5_matches.tsv                     1        0        1        0   \n",
      "3300000532_1_matches.tsv                     1        0        1        0   \n",
      "Synbio_Functional_Profile_matches.tsv        0        0        1        0   \n",
      "\n",
      "                                       1.1.1.5  1.1.1.6  1.1.1.7  1.1.1.8  \\\n",
      "Name_of_Genome                                                              \n",
      "2162886007_5_matches.tsv                     0        0        0        0   \n",
      "3300000532_1_matches.tsv                     0        0        0        0   \n",
      "Synbio_Functional_Profile_matches.tsv        0        0        0        0   \n",
      "\n",
      "                                       1.1.1.9  1.1.1.10  ...  7.6.2.7  \\\n",
      "Name_of_Genome                                            ...            \n",
      "2162886007_5_matches.tsv                     0         1  ...        0   \n",
      "3300000532_1_matches.tsv                     0         0  ...        0   \n",
      "Synbio_Functional_Profile_matches.tsv        0         0  ...        0   \n",
      "\n",
      "                                       7.6.2.8  7.6.2.9  7.6.2.10  7.6.2.11  \\\n",
      "Name_of_Genome                                                                \n",
      "2162886007_5_matches.tsv                     0        0         1         1   \n",
      "3300000532_1_matches.tsv                     1        0         1         1   \n",
      "Synbio_Functional_Profile_matches.tsv        1        0         0         0   \n",
      "\n",
      "                                       7.6.2.12  7.6.2.13  7.6.2.14  7.6.2.15  \\\n",
      "Name_of_Genome                                                                  \n",
      "2162886007_5_matches.tsv                      1         0         0         0   \n",
      "3300000532_1_matches.tsv                      0         0         0         0   \n",
      "Synbio_Functional_Profile_matches.tsv         0         0         0         0   \n",
      "\n",
      "                                       7.6.2.16  \n",
      "Name_of_Genome                                   \n",
      "2162886007_5_matches.tsv                      0  \n",
      "3300000532_1_matches.tsv                      0  \n",
      "Synbio_Functional_Profile_matches.tsv         0  \n",
      "\n",
      "[3 rows x 8236 columns]\n",
      "Shape of the entire distance matrix is:  (3, 3)\n",
      "           0          1          2\n",
      "0   0.000000  24.779023  25.903668\n",
      "1  24.779023   0.000000  24.020824\n",
      "2  25.903668  24.020824   0.000000\n",
      "Distance matrix is downloaded\n",
      "                          Name_of_Genome\n",
      "0               2162886007_5_matches.tsv\n",
      "1               3300000532_1_matches.tsv\n",
      "2  Synbio_Functional_Profile_matches.tsv\n",
      "Synbio_Functional_Profile_matches.tsv\n"
     ]
    }
   ],
   "source": [
    "# synbio = '/home/anna/Documents/JGI_soil_genomes/Synbio/'\n",
    "# name = 'Synbio_Functional_Profile'\n",
    "# desired_location2 = '/home/anna/Documents/JGI_soil_genomes'\n",
    "# synbio binary is the synbio functional profile. \n",
    "[distance_list_for_synbio, new_loc ]= pass_to_distance(synbio_binary, name, desired_location2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Distance Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
